# üìí Data Science & Machine Learning Notebook Collection (English + ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)

> For my own learning, in my own words, in my own way! | ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶∂‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡ßü, ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶ï‡¶∞‡ßá!

---

## üìñ Table of Contents | ‡¶∏‡ßÇ‡¶ö‡¶ø‡¶™‡¶§‡ßç‡¶∞

-   Quick Start | ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶∂‡ßÅ‡¶∞‡ßÅ
-   How to Use | ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßá‡¶®
-   Expanded Notes by Folder | ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶®‡ßã‡¶ü
-   Learning Tips | ‡¶∂‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶ü‡¶ø‡¶™‡¶∏
-   FAQ & Resources | ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ì ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏

---

## Quick Start | ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶∂‡ßÅ‡¶∞‡ßÅ

-   Python 3.7+
-   Jupyter Lab/Notebook
-   Libraries: pandas, matplotlib, seaborn, scikit-learn, pandas-profiling, gspread
-   ‡¶™‡¶æ‡¶á‡¶•‡¶® ‡ß©.‡ß≠+, ‡¶ú‡ßÅ‡¶™‡¶ø‡¶ü‡¶æ‡¶∞ ‡¶≤‡ßç‡¶Ø‡¶æ‡¶¨/‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï, pandas, matplotlib, seaborn, scikit-learn, pandas-profiling, gspread

**Install | ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®:**

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
pip install pandas jupyterlab matplotlib seaborn scikit-learn pandas-profiling gspread
jupyter lab
```

---

## How to Use | ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßá‡¶®

-   Browse folders by topic (e.g., Pandas, Linear Regression, etc.) | ‡¶ü‡¶™‡¶ø‡¶ï ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®
-   Open any `.ipynb` notebook in Jupyter | ‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã `.ipynb` ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶®
-   Each notebook has step-by-step code, explanation, and real datasets | ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï‡ßá ‡¶ß‡¶æ‡¶™‡ßá ‡¶ß‡¶æ‡¶™‡ßá ‡¶ï‡ßã‡¶°, ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ì ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü ‡¶Ü‡¶õ‡ßá

---

## Expanded Notes by Folder | ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶®‡ßã‡¶ü

### 01_pandas ‚Äî Data Handling with Pandas

**Notebook Links | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_list to dataframe.ipynb](01_pandas/01_list%20to%20dataframe.ipynb)
-   [02_excel to df.ipynb](01_pandas/02_excel%20to%20df.ipynb)
-   [03_pandas profiling.ipynb](01_pandas/03_pandas%20profiling.ipynb)
-   [04_Importing Google Sheets using Python Pandas.ipynb](01_pandas/04_Importing%20Google%20Sheets%20using%20Python%20Pandas.ipynb)

**What is Pandas? | Pandas ‡¶ï‡ßÄ?**

-   English: Pandas is a powerful Python library for data manipulation and analysis. It lets you load, clean, transform, and analyze data efficiently.
-   ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ: Pandas ‡¶π‡¶≤‡ßã ‡¶™‡¶æ‡¶á‡¶•‡¶®‡ßá‡¶∞ ‡¶∂‡¶ï‡ßç‡¶§‡¶ø‡¶∂‡¶æ‡¶≤‡ßÄ ‡¶°‡ßá‡¶ü‡¶æ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡¶ø‡¶™‡ßÅ‡¶≤‡ßá‡¶∂‡¶® ‡¶ì ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶®‡¶æ‡¶≤‡¶æ‡¶á‡¶∏‡¶ø‡¶∏ ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø‡•§ ‡¶è‡¶ü‡¶ø ‡¶¶‡¶ø‡ßü‡ßá ‡¶∏‡¶π‡¶ú‡ßá‡¶á ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡ßã‡¶°, ‡¶ï‡ßç‡¶≤‡¶ø‡¶®, ‡¶ü‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶∏‡¶´‡¶∞‡ßç‡¶Æ ‡¶ì ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü‡•§

**Key Concepts | ‡¶Æ‡ßÇ‡¶≤ ‡¶¨‡¶ø‡¶∑‡ßü:**

-   DataFrame creation from lists, dicts, CSV/Excel, Google Sheets
-   Data selection, filtering, sorting, grouping, aggregation
-   Data profiling for quick insights

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
import pandas as pd
# Load CSV | CSV ‡¶´‡¶æ‡¶á‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
df = pd.read_csv('Screen Time Data.csv')
print(df.head())  # Show first 5 rows | ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡ß´‡¶ü‡¶ø ‡¶∏‡¶æ‡¶∞‡¶ø ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®
```

**Advanced Example | ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
# Filter rows where 'Total Screen Time' > 100 | ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá 'Total Screen Time' ‡ßß‡ß¶‡ß¶-‡¶è‡¶∞ ‡¶¨‡ßá‡¶∂‡¶ø
filtered = df[df['Total Screen Time'] > 100]
print(filtered)

# Group by 'Week Day' and get average screen time | 'Week Day' ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ó‡ßú ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶ø‡¶® ‡¶ü‡¶æ‡¶á‡¶Æ
avg_by_day = df.groupby('Week Day')['Total Screen Time'].mean()
print(avg_by_day)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Use `.info()`, `.describe()`, `.shape` to understand your data | ‡¶°‡ßá‡¶ü‡¶æ ‡¶¨‡ßã‡¶ù‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
-   Try `pandas_profiling` for a quick data report | ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø `pandas_profiling` ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 02_types of var ‚Äî Types of Variables in Data Science

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_Types of Variables in Data Science.ipynb](02_types%20of%20var/01_Types%20of%20Variables%20in%20Data%20Science.ipynb)

**Understanding Data: Key Dimensions | ‡¶°‡ßá‡¶ü‡¶æ ‡¶¨‡ßã‡¶ù‡¶æ‡¶∞ ‡¶Æ‡ßÇ‡¶≤ ‡¶¶‡¶ø‡¶ï**

-   **Data Types | ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ß‡¶∞‡¶®:**
    -   Numerical (Discrete, Continuous) | ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶ó‡¶§ (‡¶°‡¶ø‡¶∏‡¶ï‡ßç‡¶∞‡¶ø‡¶ü, ‡¶ï‡¶®‡ßç‡¶ü‡¶ø‡¶®‡¶ø‡¶â‡ßü‡¶æ‡¶∏)
    -   Categorical (Nominal, Ordinal, Binary) | ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤ (‡¶®‡¶Æ‡¶ø‡¶®‡¶æ‡¶≤, ‡¶Ö‡¶∞‡ßç‡¶°‡¶ø‡¶®‡¶æ‡¶≤, ‡¶¨‡¶æ‡¶á‡¶®‡¶æ‡¶∞‡¶ø)
    -   Text, Date/Time | ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü, ‡¶§‡¶æ‡¶∞‡¶ø‡¶ñ/‡¶∏‡¶Æ‡ßü
-   **Measurement Scale | ‡¶Æ‡¶æ‡¶™‡¶æ‡¶∞ ‡¶∏‡ßç‡¶ï‡ßá‡¶≤:**
    -   Nominal, Ordinal, Interval, Ratio | ‡¶®‡¶Æ‡¶ø‡¶®‡¶æ‡¶≤, ‡¶Ö‡¶∞‡ßç‡¶°‡¶ø‡¶®‡¶æ‡¶≤, ‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤, ‡¶∞‡ßá‡¶∂‡¶ø‡¶ì
-   **Role in Analysis | ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£‡ßá ‡¶≠‡ßÇ‡¶Æ‡¶ø‡¶ï‡¶æ:**
    -   Independent (Input), Dependent (Output) | ‡¶á‡¶®‡¶™‡ßÅ‡¶ü, ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü
-   **Distribution | ‡¶°‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶ø‡¶¨‡¶ø‡¶â‡¶∂‡¶®:**
    -   Normal, Skewed, Uniform, Bimodal | ‡¶®‡¶∞‡¶Æ‡¶æ‡¶≤, ‡¶∏‡ßç‡¶ï‡¶ø‡¶â‡¶°, ‡¶á‡¶â‡¶®‡¶ø‡¶´‡¶∞‡ßç‡¶Æ, ‡¶¨‡¶æ‡¶á‡¶Æ‡ßã‡¶°‡¶æ‡¶≤

**Why it matters? | ‡¶ï‡ßá‡¶® ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£?**

-   Choosing the right statistical methods and visualizations depends on variable types.
-   ‡¶∏‡¶†‡¶ø‡¶ï ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶ì ‡¶≠‡¶ø‡¶ú‡ßç‡¶Ø‡ßÅ‡ßü‡¶æ‡¶≤‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ß‡¶∞‡¶® ‡¶ú‡¶æ‡¶®‡¶æ ‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø‡•§

**Example | ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
# Check data types | ‡¶°‡ßá‡¶ü‡¶æ ‡¶ü‡¶æ‡¶á‡¶™ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®
print(df.dtypes)
```

---

### 03_linear regression ‚Äî Linear Regression & Related Concepts

**Notebook Links | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_Regression Analysis.ipynb](03_linear%20regression/01_Regression%20Analysis.ipynb)
-   [02_Loss & Cost Functions in Linear Regression.ipynb](03_linear%20regression/02_Loss%20&%20Cost%20Functions%20in%20Linear%20Regression.ipynb)
-   [03_Coefficient of Determination(R-squared).ipynb](<03_linear%20regression/03_Coefficient%20of%20Determination(R-squared).ipynb>)
-   [04_Assignment 01 - Salary Prediction.ipynb](03_linear%20regression/04_Assignment%2001%20-%20Salary%20Prediction.ipynb)
-   [06\_ Linear Regression with Multiple Variables.ipynb](03_linear%20regression/06_%20Linear%20Regression%20with%20Multiple%20Variables.ipynb)
-   [08_Polynomial Regression.ipynb](03_linear%20regression/08_Polynomial%20Regression.ipynb)

**What is Linear Regression? | ‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶ï‡ßÄ?**

-   English: Linear Regression predicts a continuous value by fitting a straight line. Used for salary prediction, trend analysis, etc.
-   ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ: ‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ï‡¶®‡ßç‡¶ü‡¶ø‡¶®‡¶ø‡¶â‡¶Ø‡¶º‡¶æ‡¶∏ ‡¶Æ‡¶æ‡¶® ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶∞‡¶≤ ‡¶∞‡ßá‡¶ñ‡¶æ ‡¶´‡¶ø‡¶ü ‡¶ï‡¶∞‡ßá‡•§ ‡¶Ø‡ßá‡¶Æ‡¶®: ‡¶¨‡ßá‡¶§‡¶® ‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶æ‡¶®, ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶° ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£‡•§

**How does it work? | ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá?**

-   Finds weights (w) so that `y = w0 + w1x1 + ... + wnxn` best fits the data.
-   ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶´‡¶ø‡¶ü ‡¶π‡ßü ‡¶è‡¶Æ‡¶® ‡¶ì‡¶Ø‡¶º‡ßá‡¶ü ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá‡•§

**Mathematical Formula | ‡¶ó‡¶æ‡¶£‡¶ø‡¶§‡¶ø‡¶ï ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞:**
\[
MSE = \frac{1}{N} \sum (y - \hat{y})^2
\]

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)
print(model.predict([[5]]))  # Predict for 5 years experience | ‡ß´ ‡¶¨‡¶õ‡¶∞‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡ßç‡¶ü
```

**Advanced Example | ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
# Polynomial Regression | ‡¶™‡¶≤‡¶ø‡¶®‡ßã‡¶Æ‡¶ø‡ßü‡¶æ‡¶≤ ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶®
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
model.fit(X_poly, y)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Visualize your data and regression line | ‡¶°‡ßá‡¶ü‡¶æ ‡¶ì ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶≤‡¶æ‡¶á‡¶® ‡¶™‡ßç‡¶≤‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
-   Use train-test split for evaluation | ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á‡ßü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø train-test split ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 04_Feature Enginerring ‚Äî Feature Engineering

**Notebook Links | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_Label Encoder in Machine Learning.ipynb](04_Feature%20Enginerring/01_Label%20Encoder%20in%20Machine%20Learning.ipynb)
-   [02_One-Hot Encoding in Machine Learning.ipynb](04_Feature%20Enginerring/02_One-Hot%20Encoding%20in%20Machine%20Learning.ipynb)
-   [03\_ Binary Encoder Explained in Machine Learning.ipynb](04_Feature%20Enginerring/03_%20Binary%20Encoder%20Explained%20in%20Machine%20Learning.ipynb)
-   [04_Ordinal Encoder.ipynb](04_Feature%20Enginerring/04_Ordinal%20Encoder.ipynb)
-   [05\_ Min Max Normalization.ipynb](04_Feature%20Enginerring/05_%20Min%20Max%20Normalization.ipynb)
-   [06_Standardization Z-Score.ipynb](04_Feature%20Enginerring/06_Standardization%20Z-Score.ipynb)

**What is Feature Engineering? | ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶á‡¶û‡ßç‡¶ú‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶Ç ‡¶ï‡ßÄ?**

-   English: Transforming raw data into features for ML (encoding, scaling).
-   ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ: ‡¶ï‡¶æ‡¶Å‡¶ö‡¶æ ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡¶ø‡¶Ç‡ßü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡¶æ (‡¶è‡¶®‡¶ï‡ßã‡¶°‡¶ø‡¶Ç, ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶ø‡¶Ç)‡•§

**Key Techniques | ‡¶Æ‡ßÇ‡¶≤ ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï:**

-   Label, One-Hot, Binary, Ordinal encoding; Min-Max scaling; Standardization.

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['encoded'] = le.fit_transform(df['category'])  # ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø ‡¶è‡¶®‡¶ï‡ßã‡¶°‡¶ø‡¶Ç
```

**Advanced Example | ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
# One-hot encoding | ‡¶ì‡ßü‡¶æ‡¶®-‡¶π‡¶ü ‡¶è‡¶®‡¶ï‡ßã‡¶°‡¶ø‡¶Ç
import pandas as pd
dummies = pd.get_dummies(df['Area'], drop_first=True)
df = pd.concat([df, dummies], axis=1)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Always encode categorical variables before modeling | ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤ ‡¶°‡ßá‡¶ü‡¶æ ‡¶è‡¶®‡¶ï‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
-   Scale features for algorithms like KNN, SVM | KNN, SVM-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶∏‡ßç‡¶ï‡ßá‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 05_project_multiple_linear_regression ‚Äî Multiple Linear Regression Project

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_Profit Prediction using Multiple Linear Regression.ipynb](05_project_multiple_linear_regression/01_Profit%20Prediction%20using%20Multiple%20Linear%20Regression.ipynb)

**Project Overview | ‡¶™‡ßç‡¶∞‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶ì‡¶≠‡¶æ‡¶∞‡¶≠‡¶ø‡¶â:**

-   Predicting profit using multiple features (Marketing Spend, Administration, Transport, Area).
-   Data cleaning, one-hot encoding, train-test split, model fitting, scoring.

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

**Advanced Example | ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
# One-hot encoding for categorical feature
city = pd.get_dummies(X['Area'], drop_first=True)
X = X.drop('Area', axis=1)
X = pd.concat([X, city], axis=1)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Always check for nulls or zeros in data before modeling | ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶°‡ßá‡¶ü‡¶æ null ‡¶¨‡¶æ 0 ‡¶ï‡¶ø‡¶®‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 06_Gradient Descent ‚Äî Gradient Descent

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_gradient descent_linear regression.ipynb](06_Gradient%20Descent/01_gradient%20descent_linear%20regression.ipynb)

**What is Gradient Descent? | ‡¶ó‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶°‡¶ø‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü ‡¶°‡¶ø‡¶∏‡ßá‡¶®‡ßç‡¶ü ‡¶ï‡ßÄ?**

-   English: Optimization algorithm to minimize cost functions.
-   ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ: ‡¶ï‡¶∏‡ßç‡¶ü ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶Æ‡¶ø‡¶®‡¶ø‡¶Æ‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶Ö‡¶™‡ßç‡¶ü‡¶ø‡¶Æ‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶® ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶≤‡¶ó‡¶∞‡¶ø‡¶¶‡¶Æ‡•§

**Steps | ‡¶ß‡¶æ‡¶™‡¶∏‡¶Æ‡ßÇ‡¶π:**

1. Initialize parameters (e.g., m, c = 0)
2. Update using gradients
3. Repeat for set iterations

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
m, c = 0, 0
learning_rate = 0.001
for i in range(1000):
    y_pred = m*x + c
    md = -(2/n) * sum(x*(y-y_pred))
    cd = -(2/n) * sum(y-y_pred)
    m = m - learning_rate * md
    c = c - learning_rate * cd
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Choose learning rate carefully | ‡¶≤‡¶æ‡¶∞‡ßç‡¶®‡¶ø‡¶Ç ‡¶∞‡ßá‡¶ü ‡¶†‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶¶‡¶ø‡¶®

---

### 07_Decision Tree ‚Äî Decision Tree

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_Classification- Decision Tree Construction in Machine Learning.ipynb](07_Decision%20Tree/01_Classification-%20Decision%20Tree%20Construction%20in%20Machine%20Learning.ipynb)

**What is a Decision Tree? | ‡¶°‡¶ø‡¶∏‡¶ø‡¶∂‡¶® ‡¶ü‡ßç‡¶∞‡¶ø ‡¶ï‡ßÄ?**

-   English: Splits data into branches to make decisions.
-   ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ: ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡ßá ‡¶∏‡¶ø‡¶¶‡ßç‡¶ß‡¶æ‡¶®‡ßç‡¶§ ‡¶®‡ßá‡ßü‡•§

**Key Concepts | ‡¶Æ‡ßÇ‡¶≤ ‡¶¨‡¶ø‡¶∑‡ßü:**

-   Information gain, entropy, tree pruning.

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(X, y)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Prune trees to avoid overfitting | ‡¶ì‡¶≠‡¶æ‡¶∞‡¶´‡¶ø‡¶ü‡¶ø‡¶Ç ‡¶è‡¶°‡¶º‡¶æ‡¶§‡ßá ‡¶ü‡ßç‡¶∞‡¶ø ‡¶õ‡ßã‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 08_Confusion Matrix ‚Äî Confusion Matrix

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_Confusion Matrix with Python.ipynb](08_Confusion%20Matrix/01_Confusion%20Matrix%20with%20Python.ipynb)

**What is a Confusion Matrix? | ‡¶ï‡¶®‡¶´‡¶ø‡¶â‡¶∂‡¶® ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏ ‡¶ï‡ßÄ?**

-   Table to evaluate classification model performance.
-   ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶™‡¶æ‡¶∞‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏ ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á‡ßü‡ßá‡¶∞ ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤‡•§

**Metrics | ‡¶Æ‡ßá‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏:**

-   Accuracy, Precision, Recall, F1-score

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, y_pred)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Always check all metrics, not just accuracy | ‡¶∂‡ßÅ‡¶ß‡ßÅ accuracy ‡¶®‡ßü, ‡¶∏‡¶¨ ‡¶Æ‡ßá‡¶ü‡ßç‡¶∞‡¶ø‡¶ï ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®

---

### 09_K-Fold Cross Validation ‚Äî K-Fold Cross Validation

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_XGBoost- Implementing K-Fold Cross Validation with Python & XGBoost Classifier.ipynb](09_K-Fold%20Cross%20Validation/01_XGBoost-%20Implementing%20K-Fold%20Cross%20Validation%20with%20Python%20&%20XGBoost%20Classifier.ipynb)

**What is K-Fold CV? | K-Fold ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶°‡ßá‡¶∂‡¶® ‡¶ï‡ßÄ?**

-   Splits data into k parts to validate model robustness.
-   ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá k ‡¶≠‡¶æ‡¶ó‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á ‡¶ï‡¶∞‡¶æ‡•§

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Use cross-validation for robust evaluation | ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á‡ßü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø cross-validation ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 10_Logistic Regression Classification ‚Äî Logistic Regression

**Notebook Links | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_Logistic Regression in Machine Learning.ipynb](10_Logistic%20Regression%20Classification/01_Logistic%20Regression%20in%20Machine%20Learning.ipynb)
-   [customer_churn_prediction.ipynb](10_Logistic%20Regression%20Classification/customer_churn_prediction.ipynb)

**What is Logistic Regression? | ‡¶≤‡¶ú‡¶ø‡¶∏‡ßç‡¶ü‡¶ø‡¶ï ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶ï‡ßÄ?**

-   Used for binary classification (yes/no, 0/1).
-   ‡¶¶‡ßÅ‡¶á ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá‡¶∞ (‡¶π‡ßç‡¶Ø‡¶æ‡¶Å/‡¶®‡¶æ, ‡ß¶/‡ßß) ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§‡•§

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X, y)
print(logreg.predict([[2, 5000]]))  # ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶á‡¶®‡¶™‡ßÅ‡¶ü
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Use for classification, not regression | ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
-   Scale features for better results | ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶∏‡ßç‡¶ï‡ßá‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 11\_ Support Vector Machine ‚Äî SVM

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [SVM in Machine Learning.ipynb](11_%20Support%20Vector%20Machine/SVM%20in%20Machine%20Learning.ipynb)

**What is SVM? | SVM ‡¶ï‡ßÄ?**

-   Finds the best boundary (hyperplane) to separate classes.
-   ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶¨‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá‡•§

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.svm import SVC
svm = SVC()
svm.fit(X, y)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Try different kernels | ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® kernel ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 12_Predicting Heart Disease using Random Forest ‚Äî Random Forest

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [Predicting Heart Disease using Random Forest.ipynb](12_Predicting%20Heart%20Disease%20using%20Random%20Forest/Predicting%20Heart%20Disease%20using%20Random%20Forest.ipynb)

**What is Random Forest? | ‡¶∞‚Äç‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶Æ ‡¶´‡¶∞‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡ßÄ?**

-   Ensemble of decision trees for better accuracy.
-   ‡¶Ö‡¶®‡ßá‡¶ï ‡¶°‡¶ø‡¶∏‡¶ø‡¶∂‡¶® ‡¶ü‡ßç‡¶∞‡¶ø‡¶∞ ‡¶∏‡¶Æ‡¶∑‡ßç‡¶ü‡¶ø‡•§

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X, y)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Good for tabular data | ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶≠‡¶æ‡¶≤‡ßã

---

### 13_Save ML Model Pickle vs Joblib ‚Äî Model Saving

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [Save ML Model and Pickle vs Joblib.ipynb](13_Save%20ML%20Model%20Pickle%20vs%20Joblib/Save%20ML%20Model%20and%20Pickle%20vs%20Joblib.ipynb)

**What is Model Saving? | ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ï‡ßÄ?**

-   Save and load ML models for reuse.
-   ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ì ‡¶™‡ßÅ‡¶®‡¶∞‡¶æ‡ßü ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡•§

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
import joblib
joblib.dump(model, 'model.joblib')
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Use Joblib for large models | ‡¶¨‡ßú ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø Joblib ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 14_K-Nearest Neighbors Algorithm in ML ‚Äî KNN

**Notebook Links | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_K-nearest regreessor and removing outliers (gaussian distribution).ipynb](<14_K-Nearest%20Neighbors%20Algorithm%20in%20ML/01_K-nearest%20regreessor%20and%20removing%20outliers%20(gaussian%20distribution).ipynb>)
-   [02_Knn \_classifier.ipynb](14_K-Nearest%20Neighbors%20Algorithm%20in%20ML/02_Knn%20_classifier.ipynb)

**What is KNN? | KNN ‡¶ï‡ßÄ?**

-   Predicts by looking at the k closest data points.
-   ‡¶ï‡¶æ‡¶õ‡ßá‡¶∞ k ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü ‡¶¶‡ßá‡¶ñ‡ßá ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡ßá‡•§

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X, y)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Scale features for distance-based algorithms | ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶∏‡ßç‡¶ï‡ßá‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 15\_ What is CountVectorizer in Python & How CountVectorizer Work ‚Äî CountVectorizer

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [1_CountVectorizer in Python.ipynb](15_%20What%20is%20CountVectorizer%20in%20Python%20&%20How%20CountVectorizer%20Work/1_CountVectorizer%20in%20Python.ipynb)

**What is CountVectorizer? | CountVectorizer ‡¶ï‡ßÄ?**

-   Converts text to numeric features for ML.
-   ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡¶ï‡ßá ‡¶®‡¶ø‡¶â‡¶Æ‡ßá‡¶∞‡¶ø‡¶ï ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞‡•§

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X_vec = cv.fit_transform(text_data)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Try on your own text data | ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡ßü ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®

---

### 16_Naive Bayes Algorithm with Python ‚Äî Naive Bayes

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [1.Naive Bayes Algorithm with Python.ipynb](16_Naive%20Bayes%20Algorithm%20with%20Python/1.Naive%20Bayes%20Algorithm%20with%20Python.ipynb)

**What is Naive Bayes? | ‡¶®‡¶æ‡¶Ø‡¶º‡ßá‡¶≠ ‡¶¨‡ßá‡¶á‡¶ú ‡¶ï‡ßÄ?**

-   Probabilistic classifier based on Bayes' theorem.
-   Bayes' theorem ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡ßç‡¶Ø‡¶§‡¶æ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶æ‡¶Ø‡¶º‡¶æ‡¶∞‡•§

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(X, y)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Good for text data | ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶≠‡¶æ‡¶≤‡ßã

---

### 17*NLP Project* Spam eMail Detection with Naive Bayes Classifiers ‚Äî NLP Spam Detection

**Notebook Link | ‡¶®‡ßã‡¶ü‡¶¨‡ßÅ‡¶ï ‡¶≤‡¶ø‡¶Ç‡¶ï:**

-   [01_Spam eMail Detection with Naive Bayes Classifiers.ipynb](17_NLP%20Project_%20Spam%20eMail%20Detection%20with%20Naive%20Bayes%20Classifiers/01_Spam%20eMail%20Detection%20with%20Naive%20Bayes%20Classifiers.ipynb)

**What is NLP Spam Detection? | NLP ‡¶∏‡ßç‡¶™‡ßç‡¶Ø‡¶æ‡¶Æ ‡¶°‡¶ø‡¶ü‡ßá‡¶ï‡¶∂‡¶® ‡¶ï‡ßÄ?**

-   Detects spam emails using NLP and Naive Bayes.
-   NLP ‡¶ì Naive Bayes ‡¶¶‡¶ø‡ßü‡ßá ‡¶∏‡ßç‡¶™‡ßç‡¶Ø‡¶æ‡¶Æ ‡¶á‡¶Æ‡ßá‡¶á‡¶≤ ‡¶∂‡¶®‡¶æ‡¶ï‡ßç‡¶§‡¶ï‡¶∞‡¶£‡•§

**Key Steps | ‡¶ß‡¶æ‡¶™‡¶∏‡¶Æ‡ßÇ‡¶π:**

-   Data cleaning, feature extraction (CountVectorizer), model training, evaluation.

**Basic Example | ‡¶∏‡¶π‡¶ú ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
cv = CountVectorizer()
X_vec = cv.fit_transform(text_data)
model = MultinomialNB()
model.fit(X_vec, y)
```

**Tips | ‡¶ü‡¶ø‡¶™‡¶∏:**

-   Always preprocess text data | ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®

---

## Learning Tips | ‡¶∂‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶ü‡¶ø‡¶™‡¶∏

-   Practice by running code | ‡¶ï‡ßã‡¶° ‡¶ö‡¶æ‡¶≤‡¶ø‡¶Ø‡¶º‡ßá ‡¶™‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡¶ü‡¶ø‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®
-   Visualize data and results | ‡¶°‡ßá‡¶ü‡¶æ ‡¶ì ‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü ‡¶≠‡¶ø‡¶ú‡ßç‡¶Ø‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡ßÅ‡¶®
-   Read documentation | ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶∂‡¶® ‡¶™‡¶°‡¶º‡ßÅ‡¶®
-   Take notes | ‡¶®‡ßã‡¶ü ‡¶®‡¶ø‡¶®

---

## FAQ & Resources | ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ì ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏

-   **ModuleNotFoundError?** Install all libraries: `pip install ...` | ‡¶∏‡¶¨ ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®
-   **Jupyter won‚Äôt start?** Try `jupyter notebook` | ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶®‡¶æ ‡¶π‡¶≤‡ßá `jupyter notebook` ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
-   **Data file not found?** Run from project root | ‡¶™‡ßç‡¶∞‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶∞‡ßÅ‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®
-   **Google Sheets import?** See [Google Sheets notebook](01_pandas/04_Importing%20Google%20Sheets%20using%20Python%20Pandas.ipynb)

**Resources | ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏:**

-   [Pandas Docs](https://pandas.pydata.org/docs/)
-   [Scikit-learn Docs](https://scikit-learn.org/stable/documentation.html)
-   [Matplotlib Docs](https://matplotlib.org/stable/users/index.html)
-   [Kaggle Datasets](https://www.kaggle.com/datasets)
-   [Google Colab](https://colab.research.google.com/)

---

**Happy Learning! | ‡¶∂‡ßÅ‡¶≠‡¶ï‡¶æ‡¶Æ‡¶®‡¶æ! üöÄ**
